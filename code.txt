import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_auc_score, precision_recall_curve, auc, roc_curve
from sklearn.preprocessing import LabelEncoder, StandardScaler
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
import joblib
import warnings
import os
import time
from datetime import datetime
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

def create_output_directory():
    """Create timestamped output directory for model artifacts"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"model_output_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(os.path.join(output_dir, "plots"), exist_ok=True)
    return output_dir

def load_data(train_path="train.csv", test_path="test.csv"):
    """Load and prepare the datasets"""
    try:
        train = pd.read_csv(train_path)
        test = pd.read_csv(test_path)
        print(f"Train shape: {train.shape}, Test shape: {test.shape}")
        return train, test
    except FileNotFoundError:
        print("Data files not found. Using dummy data for example purposes.")
        # For demonstration only when files aren't available
        return None, None

def check_data_quality(df, name="dataset"):
    """Check data quality issues including missing values and outliers"""
    print(f"\n--- Data Quality Check for {name} ---")
    
    # Missing values
    missing = df.isnull().sum()
    missing_pct = (missing / len(df)) * 100
    
    if missing.sum() > 0:
        print("\nMissing values detected:")
        missing_df = pd.DataFrame({
            'Missing Values': missing[missing > 0],
            'Percentage': missing_pct[missing > 0]
        }).sort_values('Missing Values', ascending=False)
        print(missing_df)
    else:
        print("No missing values detected")
    
    # Check for outliers in numerical columns
    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
    if 'uniqueid' in numerical_cols:
        numerical_cols.remove('uniqueid')
    
    print("\nNumerical features statistics:")
    stats = df[numerical_cols].describe().T
    stats['IQR'] = stats['75%'] - stats['25%']
    stats['lower_bound'] = stats['25%'] - 1.5 * stats['IQR']
    stats['upper_bound'] = stats['75%'] + 1.5 * stats['IQR']
    print(stats[['min', 'max', 'mean', 'std', 'IQR', 'lower_bound', 'upper_bound']])
    
    # Check for potential outliers
    for col in numerical_cols:
        q1 = df[col].quantile(0.25)
        q3 = df[col].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]
        if len(outliers) > 0:
            print(f"\nPotential outliers in {col}: {len(outliers)} values ({len(outliers)/len(df)*100:.2f}%)")
    
    return missing.sum() > 0

def analyze_data(df, output_dir=None):
    """Analyze the dataset to understand its structure and distributions"""
    print("\n--- Data Analysis ---")
    print(f"Shape: {df.shape}")
    print("\nData Types:")
    print(df.dtypes)
    
    # Examine target distribution if present
    if 'bank_account' in df.columns:
        print("\nTarget distribution:")
        target_counts = df['bank_account'].value_counts(normalize=True)
        print(target_counts)
        
        # Visualize target distribution
        plt.figure(figsize=(8, 5))
        sns.countplot(x='bank_account', data=df)
        plt.title('Target Distribution')
        plt.ylabel('Count')
        if output_dir:
            plt.savefig(os.path.join(output_dir, "plots", "target_distribution.png"))
            plt.close()
        else:
            plt.show()
    
    # Check distributions of numerical features
    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
    if 'uniqueid' in numerical_cols:
        numerical_cols.remove('uniqueid')
    
    # Visualize distributions of numerical features
    if len(numerical_cols) > 0:
        nrows = (len(numerical_cols) + 1) // 2
        fig, axes = plt.subplots(nrows, 2, figsize=(14, 3*nrows))
        axes = axes.flatten()
        
        for i, col in enumerate(numerical_cols):
            if i < len(axes):
                sns.histplot(df[col], ax=axes[i], kde=True)
                axes[i].set_title(f'Distribution of {col}')
        
        plt.tight_layout()
        if output_dir:
            plt.savefig(os.path.join(output_dir, "plots", "numerical_distributions.png"))
            plt.close()
        else:
            plt.show()
    
    # Check distributions of categorical features
    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
    if 'uniqueid' in categorical_cols:
        categorical_cols.remove('uniqueid')
    if 'bank_account' in categorical_cols:
        categorical_cols.remove('bank_account')
    
    # Return features by type for preprocessing
    all_categorical_cols = categorical_cols.copy()
    all_numerical_cols = numerical_cols.copy()
    
    # For larger categoricals, create summary plots
    for col in categorical_cols:
        value_counts = df[col].value_counts()
        if len(value_counts) > 10:  # Too many categories to plot individually
            print(f"\nTop 10 values for {col}:")
            print(value_counts.head(10))
            continue
            
        plt.figure(figsize=(10, 5))
        sns.countplot(y=col, data=df, order=value_counts.index)
        plt.title(f'Distribution of {col}')
        plt.tight_layout()
        if output_dir:
            plt.savefig(os.path.join(output_dir, "plots", f"categ_dist_{col}.png"))
            plt.close()
        else:
            plt.show()
    
    # Check correlation between numerical features
    if len(numerical_cols) > 1:
        plt.figure(figsize=(10, 8))
        corr_matrix = df[numerical_cols].corr()
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
        plt.title('Correlation Matrix of Numerical Features')
        if output_dir:
            plt.savefig(os.path.join(output_dir, "plots", "correlation_matrix.png"))
            plt.close()
        else:
            plt.show()
    
    return all_categorical_cols, all_numerical_cols

def create_preprocessing_pipeline(categorical_cols, numerical_cols):
    """Create a preprocessing pipeline with imputation and encoding"""
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ])
    
    numerical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    
    preprocessor = ColumnTransformer(
        transformers=[
            ('cat', categorical_transformer, categorical_cols),
            ('num', numerical_transformer, numerical_cols)
        ])
    
    return preprocessor

def feature_engineering(df):
    """Perform feature engineering to create new features"""
    # Create a copy to avoid modifying the original DataFrame
    df_new = df.copy()
    
    # Example: Create age groups
    if 'age_of_respondent' in df_new.columns:
        df_new['age_group'] = pd.cut(
            df_new['age_of_respondent'], 
            bins=[0, 18, 30, 45, 60, 100], 
            labels=['0-18', '19-30', '31-45', '46-60', '60+']
        )
    
    # Example: Create household size groups
    if 'household_size' in df_new.columns:
        df_new['household_size_group'] = pd.cut(
            df_new['household_size'],
            bins=[-1, 1, 3, 5, 10, 100],
            labels=['Single', 'Small', 'Medium', 'Large', 'Very Large']
        )
    
    # Example: Create interaction features if relevant columns exist
    if 'education_level' in df_new.columns and 'job_type' in df_new.columns:
        df_new['education_job'] = df_new['education_level'].astype(str) + "_" + df_new['job_type'].astype(str)
    
    # Return the dataframe with new features
    return df_new

def plot_learning_curve(model, X_train, y_train, X_val, y_val, output_dir=None):
    """Plot learning curve to evaluate model performance with increasing data size"""
    percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    train_scores = []
    val_scores = []
    
    for percentage in percentages:
        # Take a subset of the training data
        n_samples = int(X_train.shape[0] * percentage)
        X_subset = X_train[:n_samples]
        y_subset = y_train[:n_samples]
        
        # Train model on subset
        model_subset = XGBClassifier(**model.get_params())
        model_subset.fit(X_subset, y_subset)
        
        # Evaluate
        train_scores.append(accuracy_score(y_subset, model_subset.predict(X_subset)))
        val_scores.append(accuracy_score(y_val, model_subset.predict(X_val)))
    
    # Plot learning curve
    plt.figure(figsize=(10, 6))
    plt.plot(percentages, train_scores, label='Training accuracy')
    plt.plot(percentages, val_scores, label='Validation accuracy')
    plt.xlabel('Percentage of training data')
    plt.ylabel('Accuracy')
    plt.title('Learning Curve')
    plt.legend()
    plt.grid(True)
    
    if output_dir:
        plt.savefig(os.path.join(output_dir, "plots", "learning_curve.png"))
        plt.close()
    else:
        plt.show()

def evaluate_model(model, X_val, y_val, X_train=None, y_train=None, output_dir=None):
    """Evaluate the model with multiple metrics and visualizations"""
    # Predict on validation set
    y_pred = model.predict(X_val)
    y_pred_proba = model.predict_proba(X_val)[:, 1]
    
    # Calculate metrics
    accuracy = accuracy_score(y_val, y_pred)
    
    try:
        auc_score = roc_auc_score(y_val, y_pred_proba)
        # Calculate precision-recall AUC
        precision, recall, _ = precision_recall_curve(y_val, y_pred_proba)
        pr_auc = auc(recall, precision)
    except:
        auc_score = 0
        pr_auc = 0
    
    # Print metrics
    print("\n--- Model Evaluation ---")
    print(f"Validation Accuracy: {accuracy:.4f}")
    print(f"Validation ROC-AUC: {auc_score:.4f}")
    print(f"Validation PR-AUC: {pr_auc:.4f}")
    print("\nClassification Report:")
    print(classification_report(y_val, y_pred))
    
    # Calculate and display confusion matrix
    cm = confusion_matrix(y_val, y_pred, normalize='true')
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', 
                xticklabels=['No Account', 'Has Account'],
                yticklabels=['No Account', 'Has Account'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Normalized Confusion Matrix')
    plt.tight_layout()
    
    if output_dir:
        plt.savefig(os.path.join(output_dir, "plots", "confusion_matrix.png"))
        plt.close()
    else:
        plt.show()
    
    # Plot ROC curve
    fpr, tpr, _ = roc_curve(y_val, y_pred_proba)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.4f})')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    
    if output_dir:
        plt.savefig(os.path.join(output_dir, "plots", "roc_curve.png"))
        plt.close()
    else:
        plt.show()
    
    # Plot Precision-Recall curve
    plt.figure(figsize=(8, 6))
    plt.plot(recall, precision, label=f'PR Curve (AUC = {pr_auc:.4f})')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend()
    
    if output_dir:
        plt.savefig(os.path.join(output_dir, "plots", "pr_curve.png"))
        plt.close()
    else:
        plt.show()
    
    # Feature importance if training data is provided
    if X_train is not None and hasattr(model, 'feature_importances_'):
        # Get feature names from pipeline
        if hasattr(model, 'feature_names_in_'):
            feature_names = model.feature_names_in_
        else:
            feature_names = [f"feature_{i}" for i in range(X_train.shape[1])]
            
        # Plot feature importance
        plt.figure(figsize=(12, 8))
        importance = model.feature_importances_
        indices = np.argsort(importance)[-20:]  # Top 20 features
        plt.barh(range(len(indices)), importance[indices])
        plt.yticks(range(len(indices)), [feature_names[i] if i < len(feature_names) else f"feature_{i}" for i in indices])
        plt.xlabel('Feature Importance')
        plt.title('Top 20 Important Features')
        plt.tight_layout()
        
        if output_dir:
            plt.savefig(os.path.join(output_dir, "plots", "feature_importance.png"))
            plt.close()
        else:
            plt.show()
            
    # Performance at different thresholds
    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
    metrics = []
    
    for threshold in thresholds:
        y_pred_t = (y_pred_proba >= threshold).astype(int)
        tp = np.sum((y_val == 1) & (y_pred_t == 1))
        tn = np.sum((y_val == 0) & (y_pred_t == 0))
        fp = np.sum((y_val == 0) & (y_pred_t == 1))
        fn = np.sum((y_val == 1) & (y_pred_t == 0))
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        accuracy = (tp + tn) / (tp + tn + fp + fn)
        
        metrics.append({
            'threshold': threshold,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'accuracy': accuracy
        })
    
    metrics_df = pd.DataFrame(metrics)
    print("\nPerformance at different thresholds:")
    print(metrics_df)
    
    # Plot metrics at different thresholds
    plt.figure(figsize=(10, 6))
    plt.plot(metrics_df['threshold'], metrics_df['precision'], label='Precision')
    plt.plot(metrics_df['threshold'], metrics_df['recall'], label='Recall')
    plt.plot(metrics_df['threshold'], metrics_df['f1'], label='F1 Score')
    plt.plot(metrics_df['threshold'], metrics_df['accuracy'], label='Accuracy')
    plt.xlabel('Threshold')
    plt.ylabel('Score')
    plt.title('Metrics at Different Thresholds')
    plt.legend()
    plt.grid(True)
    
    if output_dir:
        plt.savefig(os.path.join(output_dir, "plots", "threshold_metrics.png"))
        plt.close()
    else:
        plt.show()
    
    return {
        'accuracy': accuracy,
        'roc_auc': auc_score,
        'pr_auc': pr_auc,
        'metrics_by_threshold': metrics_df
    }

def find_optimal_threshold(y_val, y_pred_proba):
    """Find the optimal threshold that maximizes F1 score"""
    thresholds = np.arange(0.1, 0.9, 0.01)
    f1_scores = []
    
    for threshold in thresholds:
        y_pred = (y_pred_proba >= threshold).astype(int)
        tp = np.sum((y_val == 1) & (y_pred == 1))
        fp = np.sum((y_val == 0) & (y_pred == 1))
        fn = np.sum((y_val == 1) & (y_pred == 0))
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        f1_scores.append(f1)
    
    optimal_idx = np.argmax(f1_scores)
    optimal_threshold = thresholds[optimal_idx]
    optimal_f1 = f1_scores[optimal_idx]
    
    print(f"Optimal threshold: {optimal_threshold:.2f} (F1: {optimal_f1:.4f})")
    return optimal_threshold

def run_cross_validation(X, y, best_params, preprocessor, output_dir=None):
    """Perform cross-validation with the best parameters"""
    print("\n--- Cross-validation with Best Parameters ---")
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)
    cv_scores = {'accuracy': [], 'roc_auc': [], 'pr_auc': []}
    
    fold = 1
    for train_idx, val_idx in cv.split(X, y):
        print(f"\nFold {fold}/5")
        X_fold_train, X_fold_val = X[train_idx], X[val_idx]
        y_fold_train, y_fold_val = y[train_idx], y[val_idx]
        
        # Apply SMOTE only to training fold
        smote = SMOTE(random_state=RANDOM_STATE)
        X_fold_train_res, y_fold_train_res = smote.fit_resample(X_fold_train, y_fold_train)
        
        # Train model on resampled data
        fold_model = XGBClassifier(**best_params, objective='binary:logistic', eval_metric='logloss', 
                                  use_label_encoder=False, random_state=RANDOM_STATE)
        fold_model.fit(X_fold_train_res, y_fold_train_res)
        
        # Evaluate on validation fold
        y_pred = fold_model.predict(X_fold_val)
        y_prob = fold_model.predict_proba(X_fold_val)[:, 1]
        
        # Calculate metrics
        fold_accuracy = accuracy_score(y_fold_val, y_pred)
        cv_scores['accuracy'].append(fold_accuracy)
        print(f"Fold {fold} Accuracy: {fold_accuracy:.4f}")
        
        try:
            fold_roc_auc = roc_auc_score(y_fold_val, y_prob)
            cv_scores['roc_auc'].append(fold_roc_auc)
            print(f"Fold {fold} ROC-AUC: {fold_roc_auc:.4f}")
            
            precision, recall, _ = precision_recall_curve(y_fold_val, y_prob)
            fold_pr_auc = auc(recall, precision)
            cv_scores['pr_auc'].append(fold_pr_auc)
            print(f"Fold {fold} PR-AUC: {fold_pr_auc:.4f}")
        except:
            cv_scores['roc_auc'].append(0)
            cv_scores['pr_auc'].append(0)
        
        fold += 1
    
    # Print cross-validation results
    print("\nCross-validation summary:")
    for metric, scores in cv_scores.items():
        print(f"{metric}: {np.mean(scores):.4f} ± {np.std(scores):.4f}")
    
    # Save CV results to file
    if output_dir:
        cv_results = {
            metric: {"mean": np.mean(scores), "std": np.std(scores), "values": scores}
            for metric, scores in cv_scores.items()
        }
        with open(os.path.join(output_dir, "cv_results.txt"), "w") as f:
            for metric, stats in cv_results.items():
                f.write(f"{metric}: {stats['mean']:.4f} ± {stats['std']:.4f}\n")
                f.write(f"Individual fold scores: {stats['values']}\n\n")
    
    return cv_scores

def main():
    start_time = time.time()
    
    # Create output directory
    output_dir = create_output_directory()
    print(f"Output directory created: {output_dir}")
    
    # Configure logging to file
    log_file = os.path.join(output_dir, "model_log.txt")
    
    # Load data
    train, test = load_data()
    
    # If data couldn't be loaded, exit
    if train is None or test is None:
        print("Please provide valid training and test data files")
        return
    
    # Check data quality
    has_missing_train = check_data_quality(train, "training data")
    if test is not None:
        has_missing_test = check_data_quality(test, "test data")
    
    # Analyze data
    categorical_cols, numerical_cols = analyze_data(train, output_dir)
    
    # Feature engineering
    print("\n--- Performing Feature Engineering ---")
    train_fe = feature_engineering(train)
    test_fe = feature_engineering(test)
    
    # Update column lists after feature engineering
    new_categorical_cols = train_fe.select_dtypes(include=['object', 'category']).columns.tolist()
    if 'uniqueid' in new_categorical_cols:
        new_categorical_cols.remove('uniqueid')
    if 'bank_account' in new_categorical_cols:
        new_categorical_cols.remove('bank_account')
        
    new_numerical_cols = train_fe.select_dtypes(include=['int64', 'float64']).columns.tolist()
    if 'uniqueid' in new_numerical_cols:
        new_numerical_cols.remove('uniqueid')
    
    # Create target variable
    y = train_fe['bank_account'].copy()
    le = LabelEncoder()
    y = le.fit_transform(y)
    
    # Create preprocessor
    print("\n--- Creating Preprocessing Pipeline ---")
    preprocessor = create_preprocessing_pipeline(new_categorical_cols, new_numerical_cols)
    
    # Create and save the preprocessing pipeline
    X_train = train_fe.drop(['uniqueid', 'bank_account'], axis=1)
    
    # Split data
    X_train_raw, X_val_raw, y_train, y_val = train_test_split(
        X_train, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y
    )
    
    # Preprocess training and validation data
    print("Fitting preprocessor on training data...")
    X_train_processed = preprocessor.fit_transform(X_train_raw)
    X_val_processed = preprocessor.transform(X_val_raw)
    
    # Save preprocessor for later use
    joblib.dump(preprocessor, os.path.join(output_dir, 'preprocessor.pkl'))
    
    # Handle class imbalance using SMOTE
    print("\n--- Applying SMOTE to Address Class Imbalance ---")
    smote = SMOTE(random_state=RANDOM_STATE)
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)
    
    print(f"Training data shape after SMOTE: {X_train_resampled.shape}")
    print(f"Class distribution after SMOTE: {np.bincount(y_train_resampled)}")
    
    # Define parameter search space
    param_dist = {
        'learning_rate': [0.01, 0.05, 0.1, 0.2],
        'max_depth': [3, 4, 5, 6, 7],
        'min_child_weight': [1, 3, 5, 7],
        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],
        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],
        'n_estimators': [50, 100, 150, 200, 300],
        'gamma': [0, 0.1, 0.2, 0.3],
        'reg_alpha': [0, 0.1, 1, 10],
        'reg_lambda': [0, 0.1, 1, 10]
    }
    
    # Use RandomizedSearchCV for hyperparameter tuning
    print("\n--- Performing Hyperparameter Tuning with RandomizedSearchCV ---")
    search = RandomizedSearchCV(
        estimator=XGBClassifier(
            objective='binary:logistic',
            eval_metric='logloss',
            use_label_encoder=False,
            random_state=RANDOM_STATE
        ),
        param_distributions=param_dist,
        n_iter=25,  # Number of parameter settings sampled
        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),
        verbose=1,
        n_jobs=-1,
        scoring='roc_auc',  # Using ROC AUC as the optimization metric
        return_train_score=True,
        random_state=RANDOM_STATE
    )
    
    # Fit the randomized search
    search.fit(X_train_resampled, y_train_resampled, 
              eval_set=[(X_val_processed, y_val)],
              early_stopping_rounds=20,
              verbose=False)
    
    # Print best parameters and score
    print("Best parameters:", search.best_params_)
    print("Best validation score:", search.best_score_)
    
    # Save hyperparameter tuning results
    with open(os.path.join(output_dir, "hyperparameter_tuning_results.txt"), "w") as f:
        f.write(f"Best parameters: {search.best_params_}\n")
        f.write(f"Best validation score: {search.best_score_}\n\n")
        
        f.write("All results:\n")
        for i, params in enumerate(search.cv_results_['params']):
            mean_test_score = search.cv_results_['mean_test_score'][i]
            f.write(f"Parameters: {params}\n")
            f.write(f"Mean test score: {mean_test_score}\n\n")
    
    # Create and train the final model with best parameters
    print("\n--- Training Final Model with Best Parameters ---")
    best_model = XGBClassifier(
        **search.best_params_,
        objective='binary:logistic',
        eval_metric='logloss',
        use_label_encoder=False,
        random_state=RANDOM_STATE
    )
    
    # Train with early stopping
    eval_set = [(X_train_resampled, y_train_resampled), (X_val_processed, y_val)]
    best_model.fit(
        X_train_resampled, y_train_resampled,
        eval_set=eval_set,
        early_stopping_rounds=20,
        verbose=False
    )
    
    # Save the final model
    joblib.dump(best_model, os.path.join(output_dir, 'best_xgb_model.pkl'))
    joblib.dump(le, os.path.join(output_dir, 'label_encoder.pkl'))
    
    # Plot learning curve
    print("\n--- Generating Learning Curve ---")
    plot_learning_curve(best_model, X_train_resampled, y_train_resampled, X_val_processed, y_val, output_dir)
    
    # Evaluate model
    print("\n--- Evaluating Final Model ---")
    evaluation = evaluate_model(best_model, X_val_processed, y_val,